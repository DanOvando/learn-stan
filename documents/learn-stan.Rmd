---
title: "A Practical Introduction to STAN"
author: "Dan Ovando"
date: "12/27/2017"
output: 
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

```{r}
library(rstan)
library(rstanarm)
library(loo)
library(tidyverse)
library(patchwork)

```


# What's Stan and Why Use It?

Stan is a programming language designed to make statistical modeling easier and faster, especially for Bayesian estimation problems. Stan can help you estimate complex models with large numbers of parameters, and can generally do it faster than alternative like BUGS/JAGS. 

That all sounds good, but what why is that useful for me? Suppose you have a hierarchical ecological modeling problem with data clustered by space, time, and species, such as estimating the effect of ocean temperatures on coral growth. You can use Stan to fit that model (and it will likely be faster than BUGS/JAGS if you're already used to working in those platforms). Suppose just want to ues informative priors to help fit a growth model. You can use Stan for that. Suppose you just prefer Bayesian analysis and want to run a simple multiple regression. Stan can do that. 

The purpose of this document is not to illustrate or debate Bayesian analysis, but rather assumes a general knowledge and interest in model fitting, and instead seeks to provide a path to get started using Stan for the practical quantiative researcher. This is not a deep dive into the inner workings of Stan and model fitting - explanations and examples are designed to try and help people get the hang of using Stan. There are many great resources to understand the inner workings of Stan and provide higher-level examples; this is intended as a bridge to help get people there. 

## How does Stan work?

Stan is a programming language that allows you to write and fit models. It is also "compiled language", meaning that you have to write a model, then compile it to run it. This is unlike interpreted languages like R that let you more or less run code as you go. This can be a pain if you're not used to that sort of thing, but it helps make Stan much faster than saying writing the same model in R. We'll get into this later. 


Stan's key feature are a series of tools for numerical model fitting, i.e. algorithms that help you can't be fit by analytical methods, or simpler algorithms like those behind `optim`. While Stan provides a few different mode fitting algorithms, we're going to focus on the markov chain monte carlo (MCMC) based methods here. 

MCMC's are an incerdibly useful class of algorithms that can be used to fit complex models provide Bayesian inference to statistical problems. 

An MCMC works more or less like this


1. Pick some initial parameters

2.  See how well those parameters fit the data (Calculate the posterior probability of those parameters given your data, the likelihood, and priors)

3. Pick a new set of parameters through some function. 

4. Accept or reject the new parameters by some function proportional to how much the new parameters improve the model fit.

5. Repeat the process many many times 

This simple algorithm can be shown to always converge on an approximation of the posterior probability distribution of the model eventually. The challenge then is mostly in designing functions for selecting and accepting/rejecting new values that will help the algorithm converge in your lifetime. 

That's where Stan comes in. If you write the model, Stan has a number of built in algorithms for helping you use MCMC to fit and diagnose that model quickly and efficiently. It's beyond the scope of this article or my ability to explain perfectly how it does this, but @Monahan2016 provides a really great summary of the mathematics behind Stan's algorithms, that I will now butcher. 


Stan makes use of two main tools to efficiently solve Bayesian problems:  Hamiltonian Monte Carlo (HMC) and the no-U-turn sampler (NUTS). A simple MCMC might choose a new parameter value by drawing from a multivariate normal distribution centered on the last parameter value, with some tuned or supplied covariance matrix. This means that each new parameter value is likely to be highly correlated with the last paramete value, requiring you to draw a large number of samples and then "thin" the samples to create independent draws from the posterior. 

HMC and NUTS works a little differently. The details are very complicated, and I recommend reading more on this if you're interested (see @Monnahan2016), but the good news is someone else worked out all the details so that you can use it! But it's good to have a rough sense of what's going on. 

Think of the posterior probability of your model like a small planet with peaks and valleys. The of HMC/NUTS is a an astronaut that lands on this planet, and the astronauts goal is to find the highest peaks in the landscape. But, there's a catch. The astronaut is blindfolded, but has a sensor that tells her three things: her elevation, her distance from her starting point, and her "energy". Her energy is the sum of her potential and kinetic energy, and you can overlay isoclines of energy over this surface. So, she could just systematically walk across the landscape, taking an elevation measurement at each and every point. This would work, but would take forever if the landscape is at all big. So instead, she does something like this. She starts traveling in a random direction, with the only rule being she has to keep track of her energy monitor and more or less stay on the same energy isocline. She can then decide to take tiny little steps, or huge gigantic leaps (remember, it's Mars so the gravity is much lower). If she takes giant leaps, she covers the ground really quickly, but can also really easily deviate off of her energy isocline. If she takes tiny steps, it's easy to stay on the energy isocline, but takes forever to travel very far. She picks a step size, and starts traveling, keeping track of her distance from her initial starting point, only stopping when her distance from her initial starting point starts to decrease (a U-turn, i.e. when the path she's on doubles back on itself), or when she deviates off of her energy isocline, or when she has traveled as far as she's willing to go along any one isocline. Once she's stopped, she looks at her altimeter. If she's higher than she was when she started, she marks that spot as a possible peak. If she's lower than where she started, she marks that spot as part of a peak with probability proportional to how much lower she is than her starting point. If she decides to mark that spot, she then starts a new path from that point. If she doesn't mark that spot, she returns to her original position, marks that original position again, and tries a new path. 

This leaves really one key decision for the astronaut to make: how big should her stepsize be? She makes this decision by tallying the average rate at which she accepts a new position, and she knows from some smart people that an 80% acceptance rate is reflective of a good balance of of taking big enough steps to be efficient but not so large that you miss important features of the landscape. So, she keeps trying new step sizes until she hits that target acceptance rate. 

There's a lot in here then, but to translate this back into practical terms, this means that for the most part, the only key decision that you, the analyst, needs to make in implementing the HMC/NUTS algorithm is what your target acceptance rate is. Stan then does the work for you of finding the right step size to achieve that target acceptance rate during it's "warmup" period. Once it's done with that, it enters the "sampling period", holding that same step size constant.

We'll get into the implementation details of this later, but that gives a general idea of what's happening when you call Stan. This has a few really nice features. It's more efficient than say an MCMC, and the HMC process means that you don't have to thin anymore, since the HMC itself should more or less pick values that are largely independent of the last value (though this isn't always true, and sometimes some thinning is needed). This saves you that annoying step in other Monte-Carlo based approached of having to run a boatload of samples and then "waste" them by thinning them down (though HMC still discards a lot of steps in the process). But, it makes it much easier to set a target number of samples from the posterior. In addition, programs such as BUGs like to try and take advantage of Gibbs sampling to speed things up, which requires conjugate priors, giving you an incentive to select conjugate priors for your model. Unfortunately, nature is rarely so kind; we should pick distributions based off reasonable hypotheses about the state of the world, not computationel efficienty. Stan (i.e HMC) doesn't care at all whether you are using conjugate priors, encouraging you to find the right model, not the most mathematically convenient model. 

So, this all means that you can focus on writing your model, and leave the complicated tuning and running of HMC to NUTS. 

## A Note on Divergences

HMC uses a process called the "leapfrog integrator" to draw a sketch of the posterior probability surface, since for all but the most trivial model cases there is no analytical solution. The nice thing is, failures in this integrator are identified by "divergences", which basically mean that the sampler has deviated from that enery isocline we talked about before. So, the model is going along at say an average energy of 10, and then all of a sudden goes to 20, 40, 100, etc. This tells NUTS that something has gone wrong, and it abandons ship. This can be a sign of two things. Either you just need to decrease the step size (which you do by increasing the target acceptance rate), or it can be a sign that there's a problem in the design of your model. In the later case, this can sometimes be solved by just reparametrizing your model (which we'll touch on later), but in other cases can be a sign of a fundamental problem in your model (e.g. a population model that goes negative and as a result produces Inf or NA values in your posterior probability). 

Understanding and dealing with divergences is a bit of an art, but it's important to note that Stan will warn you if divergences pop up, and it's something you have to look into. Divergences mean that the model didn't correctly survey the posterior probability and so your results may by suspect. 

# Filling your Stan Toolbox

A few general notes on Stan syntax are important to note though. 

Stan is based off of C++, but is written differently. It has a bunch of features that make it a bit easier to deal with than C++, but also means that unlike other frameworks like TMB, you can't just google the C++ solution to any given problem. 

A Stan model in a .stan file is broken into a number of "blocks", each of which define a particular part of the model. There are several different possible blocks, but to start with we're going to work with the three that every model has to have: `data`, `parameters`, and `model`

```{stan, eval = F, output.var="ex1"}
/* 
You can do long blocks of comments
like this
*/


data{
// load in data
}

parameters{
// define parameters the model is trying to estimate

}
model{

// the posterior probability function

}


```

You need to preallocate space for any variable that you include in your model, by declaring its type and its size. These declarations need to all happen at the start of each block. Stan uses what is called "Strong typing", meaning basically that it is very strict in terms of what you can do to different kinds of data, and if you try and treat say an array of real numbers like a vector of real numbers it will blow up. 

So, to start with our data section. Suppose we have *N* observations of *Y* data, where N is an integer (the number of observations), and Y is a vector of real numbers. We would declare those like 


```{stan, eval = F, output.var="dataex"}

data{

int N; // the number of observations

vector[N] Y; //

}

```

This tells Stan that we have an integet object N, and we have a vector Y of length N. 

Notice that like C++, every statement needs to be closed with a semi-colon `;`, and that comments are marked by `\\`. 

You can also declare arrays using a very confusing different in structure

```{stan, eval = F, output.var="dataex"}

data{

int N; // the number of observations

vector[N] Y; //

real X[N];

}

```

In this case, Y is a vector of size [N,1] (Stan treates Vectors as a matrix based data type)

and X is a an array of 10 1 dimensional objects. That is a pretty confusing distinction, the main difference being that 

  - vectors allow vector/matrix algebra. You can't do this with arrays
  
  - arrays allow for integer storage. So, if for example you want to pass a bunch of
  indicies showing which columns correspond to different subgroups, you need to use `int index[n];` would prodice an array of length n each storing one integer
  
To give a few illustrations of this

```{stan, eval = F, output.var="dataex"}

real x[10];

real y[10];

real z[10];

z = x * y

```

Would not work
  

```{stan, eval = F, output.var="dataex"}

vector[10] x;

vector[10] y;

vector[10] z;

z = x * y

```

Also does not work! Why not???

By default, Stan goes with matrix multiplication, and x and y are both [10,1] matrices, which can't be matrix multiplied together. You can get Stan to do element-wise operations by using .* (or ./ for division). 

So this would work

```{stan, eval = F, output.var="dataex"}

vector[10] x;

vector[10] y;

vector[10] z;

z = x .* y

```


One really nice thing about Stan, as opposed to straight up C++, is that it allows for pretty easy indexing. 

This works

```{stan, eval = F, output.var="dataex"}

vector[10] x;

vector[10] y;

vector[5] z;

z = x[1:5] .* y[1:5];

```

As does this


```{stan, eval = F, output.var="dataex"}

vector[10] x;

vector[10] y;

vector[5] z;

z = x[1:5] .* y[1:5];

```

As does this

```{stan, eval = F, output.var="dataex"}

vector[10] x;

vector[10] y;

int i[5];

vector[5] z;

i = {1,2,3,4,7}

z = x[i] .* y[i];

```


If things seem like they should be working and they're not, 9 times out of 10 it's a problem with these kinds of things (e.g. trying to multiply an array times a vector). One nice feature of running models through Rstudio is if you open up your `.stan` script in Rstudio, inside the IDE there's a little "check" button in the righthand cornder of the script that will catch a lot of that stuff. It won't tell you where the problem is, but it will let you know that your .stan file won't run as written. 

Ther are a ton of other details out there in terms of structuring and manipulating data, and you can see chapters 3 and 4 of the official Stan documentation for very detailed description, but these examples should get you started with the kinds of operations most of us do. 

One other feature worth calling out for a moment is Stan's ability to set bounds on just about anything. For example, suppose you have data *x* that you know can only be positive. You can let Stan know this by `<lower = 0>`


```{stan, eval = F, output.var="dataex"}

data{

int n; // the number of observations

vector[n] y; //

vector<lower = 0>[n] x;

}


```

This lets stan know that X is defined on the range `[0, Inf]`

If x has a positive constraint, say 100, you guessed it `<lower = 0, upper = 100>`

When declared in the `data` block this isn't all that useful, beside causing Stan to crash if you accidentally pass data in violation of the constraints (which can be handy as a check that you haven't messed something up in you R script that prepares the data). 

Where the bounds get really useful is in the `parameters` block. For example, suppose we are estimating a standard deviation $\sigma$. We know that $\sigma$ has to be positive. So, we could do something like estimate $log(\sigma)$ and then use $exp(log(\sigma))$ in our model. 

Or, we can just declare the parameter $\sigma$ to have a lower bound of zero


```{stan, eval = F, output.var="dataex"}

parameters{

  real<lower = 0> sigma;

}

```

By doing this, Stan knows not to look for negative values of $\sigma$, and will even allow us do set normal priors on sigma 

```{stan, eval = F, output.var="dataex"}

model{
  
  sigma ~ normal(0, 2);
  
}
```

This is equivalent of saying that our prior on sigma is half normal, with standard deviation 2. 
The above example may have been a bit confusing since we haven't gotten to the `model` block yet. 

The `model` block is where we actually define our model (in terms of likelihoods and priors). 

There are a few ways to do this, but for now we'll stick with generally preferred syntax of 

`some_thing ~ some_distribution(distribution_parameters)`

So in the above example, we are saying that parameter $\sigma$ come from a normal distribution with mean 0 and standard deviation 2. Stan has support for all kinds of different distributions, and you should see the documentation for descriptions and notes on the meaning of different distribution parameters. 

Lastly, we should touch on `for`, `while`, and `if` statements. These all work more or less the same as they do in R, thankfully. 



```{stan, eval = F, output.var="dataex"}

model{

real test[10];

int n;

int N;

N = 10;

for (i in 1:10){

print(test[i])

}

n = 1;

while (n <= N) {
  
  n = n + 1; 
  
  print(n)

}

if (n == N){

print("hooray")
}

}

```


Notice the use of `print` there. `print` is a great way to see what's going on inside your code once you've got it at least syntactically correct (meaning it will compile). 

So, that's a small look at the key features of writing Stan code. There's a ton more out there, but those are the basic tools for most coding. Feel free to play around with the `scratch.stan` file to test out different behaviors. 


```{r}

beta <- 0.2

sigma <- 0.5

 x <- -1:200

data <- list(y = x * beta + rnorm(length(x), 0, sigma), n = length(x), x = x,
             z = x, g = x)

plot(x,data$y)

scratch <- stan(file = here::here('scripts', 'scratch.stan'),
                iter = 2000, 
                warmup = 1000,
                data = data)

plot(scratch)
```


# Example Model: Stock Recruitment Relationships

Now that you've hopefully developed a general idae of what Stan is and why you might like to use it, we're going to dig into actual using it. 

This is just generally good practice: before you start frantically coding you should sit down and write out your model. @Hobbs provides a great introduction on how to do this if you're not familiar with the process. 

You can certainly do your entire analysis in Stan by itself. However, every language has it's purpose, and the purpose of Stan is not fast and easy data manipulation. The good news is that Stan has clearnly interfaces with other programming languages like R and Python, allowing you to do a lot of the complex data manipulation in languages better suited to those tasks. You can then pass your processed data to Stan to do the model fitting, and then analyze your results back in say R. 

The most useful way to write a Stan model is in its own file, with the extension `.stan` (there are a few other ways but we're not going to cover them). 


We'll learn the basics of Stan by working trying to fill in this model one step at a time, using the fitting of a Beverton-Holt stock-recruitment relationship as an example. 

## Write your model

We are going use data from the RAM Legacy Stock Assessment Database to try and fit a Beverton-Holt stock recruitment relationship using the steepness parameterization from @Dorn2002 (for reaons that will become apparent). 

To back up, a stock recruit relationship's job is to say, for a given amount of observed spawning biomass in a fishery, how many new fish (recruits) enter the population. 


We can write a general BH model by

$$ R = \frac{\alpha{SSB}}{1 + \beta{SSB}}$$

Where SSB is spawning stock biomass (the biomass of reproductively mature fish in the population),  $\alpha$ is the maximum average number of recruits (new fish) possible in the population, and $\beta$ is the amount of SSB needed to produce on average $\alpha{/2}$

```{r, echo = F}

alpha <- 100

beta <- 200

data_frame(ssb = 0:1000) %>% 
  mutate(recruits = alpha * ssb / (beta + ssb)) %>% 
  ggplot(aes(ssb, recruits)) + 
  geom_line() + 
  geom_vline(aes(xintercept = beta), color = 'red') + 
  geom_hline(aes(yintercept = alpha / 2), color = 'blue')



```


The problem here is that $\beta$ is obviously very specific to each species, making it difficult to really say much about the resilience of a species based on that parameter. To that end @Mace1988 provided a reparameterization of thd BH equation using a term called "steepness" (*h*), which is more or less the slope of the stock-recruitment when SSB is 20% of max (i.e. unfished) SSB. This allows for species with vastly different stock sizes to be compared in terms of their steepness, with species with higher values of steepness being more resilient to fishing than those with lower steepness


$$ \frac{0.8(\alpha){(h){{SSB}}}}{0.2 (\alpha)(1 - h) + (h - 0.2)SSB}$$

```{r}

alpha <- 1000


# ssb <- 0:2000
# 
# h = 0.4

# recruits <- (0.8 * alpha * h * ssb) / (
#             0.2 * alpha * (1 - h) +
#               (h - 0.2) * ssb
#           )


cross_df(list(ssb = 0:1000, h = c(0.21,0.6,0.8))) %>% 
  mutate(recruits = (0.8 * alpha * h * ssb) / (
            0.2 * alpha * (1 - h) +
              (h - 0.2) * ssb
          )) %>% 
  ggplot(aes(ssb, recruits, color = factor(h))) + 
  geom_line() + 
  geom_hline(aes(yintercept = alpha), color = 'blue')

cross_df(list(ssb = 0:1000, h = c(0.2,0.6,1))) %>% 
  mutate(recruits = (0.8 * alpha * h * ssb) / (
            0.2 * alpha * (1 - h) +
              (h - 0.2) * ssb
          )) %>% 
  ggplot(aes(ssb, recruits, color = factor(h))) + 
  geom_line() + 
  scale_color_discrete(name = 'steepness')

```

Our goal then is going to be to try and estimate steepness and $\alpha$ for some real data

## Examine spawning and recruitment data


```{r}

sr_data <- read_csv(here::here("data","rlsadb_v4.25_ssb_recruits.csv")) %>% 
  set_names(tolower)

sr_plots <- sr_data %>%
  select(stockid, stocklong, r, ssb) %>% 
  na.omit() %>% 
  nest(-stockid, -stocklong) %>% 
  mutate(sr_plot = map(data, ~ ggplot(.x,aes(ssb,r)) + 
  geom_point()))

# trelliscopejs::trelliscope(sr_plots, name = 'sr_plots', panel_col = 'sr_plot', self_contained = TRUE)

```


Let's look at one stock in particular 


```{r}
sal_data <- sr_data %>% 
  filter(stockid == 'PSALMAKPSWUD') %>% 
  select(stocklong, year, ssb, r) %>% 
  na.omit()

sal_data %>% 
  ggplot(aes(ssb, r)) + 
  geom_point()

```

So, there seems to be a relationship, but it's messty. Our goal now will be to use Stan to estimate a BH curve for this stock. 

Let's start by writing out the model for this model. We have three parameters we need to estimate, steepness *h*, maximum recruitment $\alpha$, and some error term $\sigma$. 

We can write this as

$$[\alpha,h,\sigma | r] \propto [ r | \alpha,h,\sigma][\alpha][h][\sigma]  $$

Where *r* are our recruitment data. That's just a conceptual framework for the model; we can write it more clearly by specifying the model in terms of distributions. 

$$[\alpha,h,\sigma | r] \propto normal( log(r) | bh(h,\alpha),\sigma) * unif(h|0.2,1) * normal(\alpha|10*max(r),0.1*max(r)) * cauchy(\sigma|0,5)  $$

In english, this says that we believe that recruitment is a log-normal process, while specifying appropriate priors for our other parameters. E.g. we know that *h* has to be between 0.2 and 1, and it's reasonable to think that max recruitment is someting larger than the largest recruitment ever observed (**much more care needs to be taken in prior construction, this is just an example**) 


Easiest way to think about the log normal distribution is a normal distribution with a cv instead of a sigma . So, you can just calculate the cv for your data and that should be sigma in log space 

## Write our Stan Model

Now that we know what we need to model, we just have to code it. We'll work step by step through this process now over in "scripts/bh_model.stan". 

...

Now that we have our `.stan` file written, we just need to pass out data to it and fit the model. the `rstan` package makes it really easy to interface between `R` and `Stan`. 

The first step is passing data from the `R` environment to `Stan`. You remember our `DATA` blcok in our `.stan` file? We simply need to pass create a list in `R` containing named objects matching each of the entries in our `DATA` block. 

Once we've done that, we use the `stan` function to fit our model. 

There are a few options that are important to specify in the call to `stan`

  - The `file` entry specifies the path to the `.stan` file containing your model
  
  - `data` is your list of data 
  
  - `chains` specifies the number of chains used in the model fitting. Any actual model run should contain multiple chains to verify convergence, but you can start with one chain for diagnostics. If you have more than one chain, by default `stan` will run them one after another, so if your model takes a long time this can be daunting. However, you can also specify `cores`. If you set `cores` to more than 1, then `Stan` will run each chain in parallel on different cores. So, if you specify 4 chains and 4 cores, each chain will be run simultaneously on separate cores, so your run time should be about the same as 1 chain on 1 core
  
  - `warmup` is the number of model iterations dedicated to burnin/tuning/whatever you want to call it. This number defaults to half of `iter` (the total number of model iterations), but if you start to do large iteration runs (e.g. 20,000), there isn't neccesarily a need to do 10,000 warmup runs. If you've tested your model on lower iterations and diagnostics look good with say 1,000 warmup runs, there shouldn't be any problem with leaving warmup at 1,000 and setting iterations to 20,000; it just gives you a lot more samples to play with. 
  
 - `init` allows you to pass initial parameter values for each chain. This is optional, but can help **A LOT**. By default, `Stan` randomly draws numbers between -2 and 2 for initial values for each parameter. This works if you're model is reasonably centered. But, if you're working in a situation where parameters can vary wildly from that (say estimating carrying capacity for a population), this range is going to be a really bad guess if the true parameter value is in the millions. If your model is correctly written, `Stan` will get to the right result eventually, but it will take a lot longer if you feed it a really poor starting guess. There are a few different ways to set `init`, I'm just going to cover passing explicit starting guesses. `init` must by a list of list, of the general form `list(chain_1 = list(h = 0.2), chain_2 = list(h = 0.8))`. The inner lists contain are names objects for any parameters in the model. In this case, I have a paremter named `h` in the model, and I'm going to specify an intitial guess of `h` at 0.2 for the first chain, and 0.8 for the second chain. It is very important if you are manually specifying starting guesses to have different initial values for your parameters, since a test of model convergence is whether or not different chains initiated at different values reach the same result. Any parameters you do not manually specify a starting guess for `stan` goes back to the default random number between -2 and 2
 

```{r, message=F, warning=F}

warmups <- 1000

total_iterations <- 2000

max_treedepth <-  10

n_chains <-  4

n_cores <- 1

data <- list(n = nrow(sal_data),
               r = sal_data$r,
             ssb = sal_data$ssb,
               max_r = max(sal_data$r)
             )

bh_fit <- stan(file = here::here("scripts","bh_model.stan"),
                 data = data,
                 chains = n_chains,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = n_cores,
                 refresh = 250,
                 init = list(list(h = 0.4, alpha = 2 * data$max_r),
                             list(h = 0.21, alpha = 3 * data$max_r),
                             list(h = 0.8, alpha = 1 * data$max_r),
                             list(h = 0.3, alpha = .8 * data$max_r)),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))


```


## Running Diagnostics

Now that we have a model run, it's time to examine our fits. 

In my opinion, `rstanarm::launch_shinystan` is by far the best way to do this. The folks at `Stan` built a pretty amaing interface (`shinystan`) that automatically puts together a wide array of numeric and graphical diagnostics that they recommend running on `Stan` model. 

We'll walk through this together, but if you're reading this on your own, I recommend running `?launch_shinystan` and taking a look at their examples to get a feel for what it can do. 

```{r, eval = F}

rstanarm::launch_shinystan(bh_fit)

```

However, we might also want to be able to run some important diagnostics from within R, either for model comparison or inclusion in reports/publications, so we'll now look at use the fitted `stan` model in R. 


Calling `stan` creates an object of class `stanfit`

```{r}

class(bh_fit)

```

`stanfit` options are designed to interface with a few base R commands that you're use to, like `summary` and `plot` (though if you have lots of paremeters simply calling these can be pretty messy)

```{r}

summary(bh_fit)

```

The plot command is a great way to get a first glance at your fits

```{r}
plot(bh_fit)
```

Before we look at using our model though, let's take a look at a few diagnostics to try to evaluate our model fit. **DISCLAIMER** You should put a lot more thought and effort in model model dianogsis in real cases, this is just an example of accessing some of the starting points in this process. 

The first thing we can check is for the presence of "divergent" transitions (see earlier section for a reminder on what these are). Divergent transition during the sampling period of your model (the iterations after the burnin) are sign that there maybe a problem with your model. We'll talk about dealing with these later, but for now here's how to see if they happened. 


`rstan` has a few functions to check these things

```{r}

rstan::check_divergences(bh_fit)

```

We can also look manually at these diagnostics using the output of `rstan::get_sampler_params`. `get_sampler_params` returns a list with one object per chain. Each object is a matrix showing diagnostics of each of the stored iterations from the model fitting (by default `get_sampler_params` includes the warmup iterations, you can set the option `inc_warmpup = FALSE` to omit these from the report if you want)

```{r extract-diagnostics}


mack_diagnostics <- rstan::get_sampler_params(bh_fit) %>% 
  set_names(1:n_chains) %>% 
   map_df(as_data_frame,.id = 'chain') %>% 
  group_by(chain) %>% 
  mutate(iteration = 1:length(chain)) %>% 
  mutate(warmup = iteration <= warmups)
 

mack_diagnostics %>% 
  group_by(warmup, chain) %>% 
  summarise(percent_divergent = mean(divergent__ >0)) %>% 
  ggplot() +
  geom_col(aes(chain, percent_divergent, fill = warmup), position = 'dodge', color = 'black') + 
  scale_y_continuous(labels = scales::percent)

```

We see then that across all chains,  we had no divergenes during the sampline period (after the warmups), which is what we want to see!

`treedepth` is another really important thing to take a look at. Remember how the HMC algorithm works (more or less). Ideally, a new candidate draw from the parameter space is selected from a place where the likelihood bends back on itself. If you think of the posterior probability space like a circular racetrack, the sampler is a runner on that racetrack. The runner starts off on the left side of the track and starts running north, goes around the bend, and then starts running south. The HMC algorithm would stop and try a new parameter at that point, where the runner has fully turned around. So that sounds great if you've got a small track to run on. Suppose though that you are on a 10,000 mile track. Your runner is going to have to run a looooong way before things start to bend around. Or, suppose your runner is on a straight line, the runner is never going to turn around, and so the HMC algorithm would just keep running forever! That's where the `max_treedepth` option comes in. HMC will select a candidate parameter value when the parameter space bends back on itself OR when the number of steps specified by `max_treedepth` is reached. Basically, if the algorithm gets to `max_treedepth`, the runner says "phew, I'm tired, I'm stopping here", evaluates that point, and then tries again for another iteration. 

By default, `max_treedepth` is set to 10. So, we should check and make sure that our model isn't bumping up against `max_treedepth` a bunch. If it is, that means that the model is selecting candidate draws based on hitting this cap, rather than properties of the posterior probability. 


```{r}

mack_diagnostics %>% 
  ggplot(aes(iteration, treedepth__, color = chain)) + 
  geom_line() + 
  geom_hline(aes(yintercept = max_treedepth), color = 'red')

```


Looks like we're good, the treedepth of each all of our iterations was below the max_treedepth, meaning that stan was selected the parameters for that iteration. 

If you're curious, you can also see the warmup process through the `stepsize` parameter. `Stan` uses the warmup period to tune the `stepsize` parameter to achieve a target acceptance rate (specified by `adapt_delta`). You can think of stepsize like resolution. A big `stepsize` means the model will quickly cover the entire picture of the posterior, but the picture will be really fuzzy, and if the posterior probability surface has important fine scale variation, the model will miss them. A really small stepsize will produce a really high resolution picture, but it will wake a lot longer to make that picture. So, a great feature of `stan` is it uses this target acceptance rate to find the right stepsize for the model. 


```{r}

mack_diagnostics %>% 
  ggplot(aes(iteration, stepsize__, color = chain)) + 
  geom_line() 

```




There are many more diagnostics for the actual sampler, but those are two of the really critical ones. Just becasue the divergences and treedepth look good doesn't mean that your model doens't have problems that deeper diagnostics would reveal, but seeing problems in those two diagnostics should give you a huge red flag right off the bat. 

You can test these things by changing the `control` options in your call to `stan`. Try for example setting `adapt_delta = 0.5` or `max_treedepth = 2`. You'll see that you start to develop divergences in the first case, since in order to achieve that target acceptance rate `stan` sets the stepsize quite large, meaning that you miss important parts of the parameter space and create divergences. In the second case you'll start to see that the treedepth start so bump up against the `max_treedepth`. 

While these are extreme examples, this also gives an idea of a first step at fixing these problems if they pop up: if you fit a model and you get divergences, the first thing you can try is to increase `adapt_delta` (in fact, `Stan` will suggest as much to you if this happens). If you're bumping up against `max_treedepth`, increase `max_treedepth`! If either of these don't solve the problem, then you'll need to start think about the specification of your model, which we'll cover a little later. 


### Parameter Diagnostics

Now that we've taken a look at the highest-level red flags (divergences and treedepth) and satisfied ourselves that we're in the clear, we can start to diagnose individual parameter estimates. There are a lot of ways to look at the these, the most useful starting point in my opinion is to extract summary statistics on each model parameter. You can do that by `summary(my_model)$summary`. `summary` on it's own prints out summaries of each parameter, the `$summary` part allows you to access and store the data behind the printed statistics. 

```{r}
 bh_summary <- summary(bh_fit)$summary %>% 
  as.data.frame() %>% 
  mutate(variable = rownames(.)) %>% 
  select(variable, everything()) %>% 
  as_data_frame()

bh_summary
```

There are two really important diagnostic statistics hidden in this summary:

  - `n_eff`: the effective sample size

  - `Rhat`: the "Gelman and Rubin potential scale reduction statistic"
  

`n_eff` measures the effective sample size of that particular parameter. Remember that each iteration of the HMC is based off the parameter value on the previous iteration XX. Ideally though, if the algorithm works correctly, the parameter chosen at the next iteration will be independent of that early parameter value (this is what "thinning" looks to accomplish in other MCMCs, though you can also thin using HMC). If you're not doing a very efficient job at sampling the parameter space though, parameter values at a given iteration are more likely to be close to the parameter values at the last iteration. This means that these parameters aren't really independent, and so if you have 1000 draws from the posterior, you might not actually have 1000 independent samples of the parameter, but rather some smaller number of truly "independent" draws. 

So, the mack `n_eff` is the sum of the sampling iterations across all chains. In this case, we have 4 chains, with 2000 iterations, half of which are warmup, meaning we sample 1000 iterations in each chain, so the max `n_eff` possible in this case is 4000

```{r}

bh_summary %>% 
  ggplot(aes(n_eff)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = 4000), color = 'red')

```

Most of our parameters have a fairly high `n_eff`, though we see a few are somewhat lower. The `Rhat` statistic helps tell us whether these paremeters are so poorly sampled that we have a problem. More or less `Rhat` tells you whether or not each of the chains has reached a stable posterior distribution, despite starting at different starting values. Gelman recommends that `Rhat` for each parameter be less than 1.1

```{r}

bh_summary %>% 
  ggplot(aes(Rhat)) + 
  geom_histogram() + 
  geom_vline(aes(xintercept = 1.1), color = 'red')

```

Looks like we're good! However, if you're concerned effective sample size of some of your parameters, the easiest thing to do is simply increase the total number of iterations and especialyl the warmup period. E.g. in this case we could move to 4000 iterations per chain with 2000 warmup iterations. 

So now that we've checked some individual parameter diagnostics, we can take a look at our parameter estimates themselves. 


Going back to the summary we created, you'll notice that `stan` kindly calcualted mean values and credible intervals for us. 

Remember that we had three parameters in our model, steepness `h`, max recruits $\alpha$, and our standard deviation of the log recruits $\sigma$. Let's take a look at the fits for those variables

```{r}

bh_summary %>% 
  filter(variable %in% c('h','alpha','sigma')) %>% 
  ggplot() + 
  geom_linerange(aes(variable, ymin = `2.5%`,ymax = `97.5%`)) + 
  geom_crossbar(aes(variable, mean, ymin = `25%`, ymax = `75%`), fill= 'grey') + 
  facet_wrap(~variable, scales = 'free')
 
```

So, we see that most of our credible intervals are fairly tight, though our estimate of unfished recruits $\alpha$ is somewhat wide.


That's all well and good, but what we might really want to see are the actual recruits estimated by our model. `stan` is great for this as well: Since we calculated our estimates of recruitment in the `transformed parameter` block, `stan` automatically stores the values for recruitment associated with each draw from the posterior, giving us our credible intervals for our recruitment estimates as well!

Our recruitment estiamtes were stored in the `rhat` object

```{r}


rhat <- bh_summary %>% 
  filter(str_detect(variable,'rhat') & !str_detect(variable,'log') & !str_detect(variable,'pp'))

sal_data <- sal_data %>% 
  mutate(mean_rhat = rhat$mean,
         lower = rhat$`2.5%`,
         upper = rhat$`97.5%`)

sal_data %>% 
  ggplot() + 
  geom_point(aes(ssb, r)) + 
  geom_line(aes(ssb, mean_rhat)) + 
  geom_ribbon(aes(ssb, ymin = lower, ymax = upper), alpha = 0.25)

```

Looks good! So, the black line is our model's estimate of the expected recruits at a given ssb, and the grey ribbon corresponds to the 95% credibility interval around this expected value. 


If we're sticklers for the truely raw data, we can also look at the parameter values at each of our samples, rather than using `summary` to process them for us. We can get at those using `rstan::extract()`. Note: by default `rstan::extract()` excludes the warmup iterations, and reshuffles the draws. If you want to keep the draws in their original order (for example to check for autocorrelation), you can set `rstan::extract(permuted = FALSE)`, and if you want to include the warmup period `rstan::extract(inc_warmup = TRUE)`


```{r}
bh_mcmc <- bh_fit %>% 
  rstan::extract()

bh_pars <- bh_mcmc[ c('h','alpha','sigma')] %>% 
  map_df(as_data_frame, .id = 'variable')

bh_pars %>% 
  ggplot(aes(value, fill = variable)) + 
  geom_density() + 
  facet_wrap(~variable, scales = 'free') + 
  coord_flip()
  
```


### Posterior Predictive Analysis

So far, we've fit our model, checked some critical diagnostics, and examined our model fits. `Stan` also allows us to examine "posterior predictive" fits, an immensely powerful tool in diagnosing Bayesian models, and in using Bayesian models for prediction. 

A huge advantage of Bayesian modeling is that it forces us to very explicitly write out our model in terms of our beliefs about the error structures of the model. For example, here we have assumed that our observed recruitment data come from a log-normal distribution, that steepness comes from a uniform distribution on the intervel 0.2-1, etc. 

Together then, each of these distributions make up the posterior probability distribution, which `stan` helps us sample from. This also means though that we have a very clear hypothesis about the underlying process generating our data. So, if our hypothesis is right, our model should be able to generate data that looks very similar to the data we actually observed. 

In this case, our data are observed recruits. We hypothesize that these observed recruits come from a distribution. 


$$log(recruits) \sim normal(bh(h,\alpha,ssb), \sigma)$$

So, using our draws from the posterior of h, $\alpha$, $\sigma$ and our observed SSB, we can use this model to generate draws of log_recruits, and compare those to the values that we actually see. 

```{r}


pp_rhat <- bh_mcmc[ 'pp_rhat'] %>% 
  map_df(as_data_frame, .id = 'variable') %>% 
  gather(observation,value, -variable)

ggplot() + 
  geom_density(data = pp_rhat, aes(value,fill = 'Posterior Predictive'), alpha = 0.5) + 
  geom_density(data = sal_data, aes(r, fill = 'Observed'), alpha = 0.5)

```

So, we see that our data does a reasonable job of reproducing the overall shape of the observed data, a good sign! We can do a lot more with this type of analysis, for example testing the ability of the posterior predictive to estimate "test statistics" like the min, max, mean, and standard deviation of the observed data, `launch_shinystan` does several of these for you. 

We can also use the posterior predictive to use our model to make predictions. Suppose we monitor another year of data for SSB, and we want to use our model to predict the recruits that that SSB will produce. We can use the posterior predictive to generate replicates from our distribution across our posterior draws of *h*, $\alpha$, and $\sigma$. To illustrate this process, we can look at the posterior predictive distributions for the SSBs we used to fit the model 

```{r}

rhat <- bh_summary %>% 
  filter(str_detect(variable,'rhat') & !str_detect(variable,'log') & !str_detect(variable,'pp'))

pp_rhat <- bh_summary %>% 
  filter(str_detect(variable,'pp_rhat')) %>% 
  mutate(ssb = sal_data$ssb)


sal_data <- sal_data %>% 
  mutate(mean_rhat = rhat$mean,
         lower = rhat$`2.5%`,
         upper = rhat$`97.5%`)

sal_data %>% 
  ggplot() + 
  geom_point(aes(ssb, r)) + 
  geom_line(aes(ssb, mean_rhat)) + 
  geom_ribbon(aes(ssb, ymin = lower, ymax = upper), alpha = 0.25) + 
  geom_line(data = pp_rhat, aes(ssb, mean), color = 'red') +
  geom_ribbon(data = pp_rhat, aes(ssb, ymin = `2.5%`, ymax = `97.5%`), alpha = 0.25, fill = 'red') 

```

We see that this interval is much broader than our credibility intervals for the mean of the fitted values. This is because the grey shaded area is our credibility interval of the expected values of our model for the observed data. That is different though than our expected value for a new observation of SSB. 

All together then, this simple model walks us through the basic steps of using `stan` to fit models:

  1. Writing our model (in terms of likelihoods)
  
  2. Coding our model
  
  3. Passing our data from R to Stan
  
  4. Performing high-level diagnostics on our model fit (divergence, trees, Rhat, etc.)
  
  5. Examining the fitted coefficients of our model
  
  6. Examining the posterior predictive statistics
  
  7. Using our coefficients for prediction
  

Each of these steps warrants more careful consideration than we have gone through here, but this is a solid foundation to base future analysis are, that will work with simple models like this, or much more complex models, the process stays the same. 

## Model Comparison

So far, we've been focused on diagnosing our model to make sure that it has in fact converged and to understand the behavior of our model if it has. However, to put it bluntly, if you're only writing one model, you're doing it wrong. We made a huge number of assumptions in the design of this model, from the functional form of our stock-recruitment relationship, to our choices of likelihoods and priors. 

Our next step should be to test these assumptions by constructin alternative models and comparing them. Luckily, `stan` has a number of tools available to help us with model comparison. 

The Beverton-Holt model assumes a "compensatory" nature of density dependence. A simple ecological example would be habitat filling: if there is a finite amount of available habitat for recruits, once those spots fill up even if you put more and more eggs into the system the total number of recruits will stay the same. The Ricker model is a bit more flexible, and allows for "depensatory" dynamics, which basically means that the SR curve can start to bend back down. An example of this would be a canabalistic process, where once adult density (SSB) gets high enough, they start to prety on recruits and actually drive recruitment back down. 

We can use `Stan` to test the relative performance of the BH vs Ricker models in explaining the observed patterns of SSB and recruitmenbt. 

Steepness isn't an applicable parameter in the Ricker model, so we will use the form fmor @Dorn2002

$$R = \alpha * SSB * exp(-\beta*SSB)$$


```{r}

rzero <-  10

szero <- 9

h = 0.9

beta <- log(5 * h) / (0.8 * szero)

ssb <- 0:500

alpha <- log(rzero / szero) + (beta * szero)

recruits <- (ssb * exp(alpha - beta * ssb))

data_frame(ssb = ssb, recruits = recruits) %>% 
  ggplot(aes(ssb, recruits)) +
  geom_point()



```



```{r ricker}

warmups <- 2000

total_iterations <- 4000

max_treedepth <-  10

data <- list(
  n = nrow(sal_data),
  r = sal_data$r,
  ssb = sal_data$ssb,
  max_r = max(sal_data$r),
  bh = 0,
  n_sr_params = 2,
  max_h = 2,
  rec_par_mean = c(2 * max(sal_data$r),  0.5 * max(sal_data$ssb)),
  rec_par_cv = c(0.5, 0.5),
  fuck = array(2.367, dim = 1),
  wtf = 1
  )

ricker_fit <- stan(file = here::here("scripts","generic_stock_recruit.stan"),
                 data = data,
                 chains = 4,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = 1,
                 refresh = 250,
                 init = list(list(h = 0.4, rec_pars = c(2 * data$max_r, 4 * max(data$ssb))),
                             list(h = 0.21, rec_pars = c(1 * data$max_r, 10 *  max(data$ssb))),
                             list(h = 0.8, rec_pars = c(3 * data$max_r, 6 *  max(data$ssb))),
                             list(h = 0.3, rec_pars = c(4 * data$max_r, 5 *  max(data$ssb)))),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))


```


You'll notice below that I'm doing some slightly odd things with arrays in my data call. This is because `Stan` is much more finicky (or exact if you want to think of it that way) about data types. Part of the appeal of `R` from a data wrangling perspective is that it is **REALLY** forgiving about data types. R is perfectly happy to construct a vector of length 1, or a 1 by 1 matrix, and these things will more or less behave in the same way (don't hold me to that). `Stan` though is a bit pickier. If `Stan` sees a piece of data that is supposed to be a vector, and sees that it is only length 1, it says "nope, that's a scalar", and bad things happen. In this case, I have specified `rec_par_mean` and `rec_par_cv` to be real vectors in my `DATA` block, of length `n_sr_params`. But, when `n_sr_params` is 1, and `Stan` sees that `rec_par_mean` is just one number, it says OK, this is a scalar, and a scalar can't have a length, so what's this dimensions thing doing here? We get arround that by specifying that `rec_par_mean` is an array with 1 dimension (see [here](http://mc-stan.org/rstan/reference/stan.html), down above the references). 

```{r bh2}

warmups <- 2000

total_iterations <- 4000

max_treedepth <-  10

data <- list(
  n = nrow(sal_data),
  r = sal_data$r,
  ssb = sal_data$ssb,
  max_r = max(sal_data$r),
  bh = 1,
  n_sr_params = 1,
  max_h = 1,
  rec_par_mean = array(2 * max(sal_data$r), dim = 1),
  rec_par_cv = array(0.5, dim = 1)
  )

bh_fit <- stan(file = here::here("scripts","generic_stock_recruit.stan"),
                 data = data,
                 chains = 4,
                 warmup = warmups,
                 iter = total_iterations,
                 cores = 1,
                 refresh = 250,
                 init = list(list(h = 0.4, rec_pars = as.array(2 * data$max_r)),
                             list(h = 0.21, rec_pars = as.array(1 * data$max_r)),
                             list(h = 0.8, rec_pars = as.array(3 * data$max_r)),
                             list(h = 0.3, rec_pars = as.array(4 * data$max_r))),
                 control = list(max_treedepth = max_treedepth,
                                adapt_delta = 0.95))





```


```{r}
 bh_summary <- summary(bh_fit)$summary %>% 
  as.data.frame() %>% 
  mutate(variable = rownames(.)) %>% 
  select(variable, everything()) %>% 
  as_data_frame()


 ricker_summary <- summary(ricker_fit)$summary %>% 
  as.data.frame() %>% 
  mutate(variable = rownames(.)) %>% 
  select(variable, everything()) %>% 
  as_data_frame()
 
ricker_rhat <- ricker_summary %>% 
  filter(str_detect(variable,'rhat') & !str_detect(variable,'log') & !str_detect(variable,'pp')) %>% mutate(model = 'ricker',
                                                                                                            ssb = sal_data$ssb)


bh_rhat <- bh_summary %>% 
  filter(str_detect(variable,'rhat') & !str_detect(variable,'log') & !str_detect(variable,'pp')) %>% 
  mutate(model = 'bh',
         ssb = sal_data$ssb)

rhat <- ricker_rhat %>% 
  bind_rows(bh_rhat)

  rhat %>% 
  ggplot() + 
  geom_point(data = sal_data,aes(ssb, r)) + 
  geom_line(aes(ssb, mean, color = model)) + 
  geom_ribbon(aes(ssb, ymin = `2.5%`, ymax = `97.5%`, fill = model),alpha = 0.25)
```

```{r}

wtf <- ricker_summary %>% 
  slice(1:4) %>% 
  select(variable, mean) %>% 
  spread(variable, mean)

h <- wtf$h

wtf$`rec_pars[2]` <- wtf$`rec_pars[2]` * 0.1

 beta = log(5 * wtf$h) / (0.8 * wtf$`rec_pars[2]`);

      alpha = log(wtf$`rec_pars[1]` / wtf$`rec_pars[2]`) + 0.8 * beta * wtf$`rec_pars[2]`;

      rhat = sal_data$ssb * exp(alpha - beta * sal_data$ssb) 
 
 plot(sal_data$ssb, rhat)

```


We now have two alternative models, a Beverton-Holt and a Ricker SR relationship. 

As a first step we should of course conduct all the same convergenve tests for the Ricker model that we conducted for the BH model, and we can base some judegement based on those results. For example, if the posterior predictive test look much better for one model that might give us some indication that the data at least support one model over another. 

We can also use the `loo` package to try and quantitatively compare the two models. `loo` stands for leave-one-out, and the `loo` function provides a powerful interface for performing leave-one-out cross validation for Bayesian models. Basically, it tests the out-of-sample predictive accuracy of each of the models. You can think of it as an improvement over AIC/DIC for model comparison (see `?"loo-package"`). 

There are a few ways to use `loo`, but the simplest requires a bit of prep work. `loo` needs to evaluate the likelihood as a function of leaving out data. So, it needs to have access to the pure likelihood. You can either write a function to do this, which we won't cover here (see `?loo::loo`), or you can go back in your model and store the log-likelihood in the `generated quantities` block


```{stan, eval = F,  output.var="ex1"}

  vector[n] log_likelihood;

  for (i in 1:n) {

   parameter_name[i] = normal_lpdf(log_r[i] | log_rhat[i] - 0.5 * sigma^2, sigma);
    
  }


```

Once we've done this, we can extract the log-likelihood using `loo::extract_log_lik`. 

`loo::extract_log_lik()` has an option `parameter_name` that defaults to `parameter_name = "log_lik"`, but for the sake of this example we've named our log-likelood object in the `stanfit` object `parameter_name`


```{r}

log_lik_ricker <- extract_log_lik(ricker_fit, parameter_name = "log_likelihood")

```

Once we have this, we can pass the log-likelihood matrix to the `loo` function to get our diagnostics. 


```{r}

ricker_loo <- loo::loo(log_lik_ricker)
```


On their own, these values aren't too informative for us (in the same way that a lone AIC value doesn't really tell you much). 

But, we can now repeat this process with out BH model, and use `loo` to compare them. 


```{r}

log_lik_bh <- extract_log_lik(bh_fit, parameter_name = "log_likelihood")

bh_loo <- loo::loo(log_lik_bh)


```

THe output of compare is a big confusing, but basically, if `elpd_diff` is positive, that means that according to `loo`, the second model is prefered. If it's negative, the first model is preferred. So, in this case, per the `loo` criteria, there is a bit more support for the Beverton-Holt model, at least as we parameterized it here. But we can also see that `elpd_diff` is on about the same scale as the standard error `se`, giving some indication that there isn't a big difference between the models (if for example `elpd_diff` had been `-2000`, much bigger than the `se` of `0.5`, this would indicate more support for the difference). 

If you want to compare more than two models, you just pass more `loo` objects to compare!

```{r}
compare(bh_loo, ricker_loo, ricker_loo)

```

Compare conveniently orders the matrix in descending order of model performance. 

And just like that, we have a solid sketch of going from raw data, to model fits, to model comparison, using Stan and R. There is clearly a lot more work that would have to go into doing this analysis properly (e.g. we haven't done any testing of the effects of our choices for our prior distributions), but the tools we've gone over here should serve as a useful template to build off of for more complete analysis. 

# State-Space Schaefer Model

Let's see how this foundation works for a more complicated model. The Schaefer (or logistic if you will) model is commonly used as a first pass of estimating the size and status of a fished popultion if an index of abundance and a catch history are available. 

$$b_{t + 1} = b_{t} + b_{t}r(1 - \frac{ b_{t}}{k}) - c_{t}$$

This model is generally fit by relating biomass in time *t* $b_{t}$ to the index of abundance (typically catch-per-unit-effort, or cpue) in that time, through a catchability coefficient *q*

$$cpue_{t} = q{b_{t}}$$

So, writing this out, we get to 

$$[r,k,q,\sigma | cpue] \propto [cpue|sch(r,k,q,catch),\sigma][r][k][q][\sigma] $$

We commonly assume that *cpue* values are log-normally distributed, so we can write our likelihoods as

$$[r,k,q,\sigma | cpue] \propto normal(log(cpue)|log(sch(r,k,q,catch)),\sigma)normal(r| \bar{r}, \sigma_{r})]unif(k | 1000,3000)unif(q,.00001,1)normal(\sigma,0,\sigma_{sigma}]$$

Again, these are just example priors, much more care would be needed in construction of these in a real application. 

In this formulation, what assumption have we made about the nature of the errors in the system?

$$normal(log(cpue)|log(sch(r,k,q,catch)),\sigma)$$

implies that all the error in our model comes from observation. Meaning, the model as written assumes that there is a true population (and hence true cpue described by $qb_{t}$) described perfectly by our Schaefer model, but we measure those cpues imperfectly, with that imperfection quantified by the observation error $\sigma$. This is a commonly made assumption, partly since it makes the modeling much easier.

However, we could also consider *process error*, meaning that there are "errors" (or deviations) in the underlying process model as well. In the case of the Schaefer model, we could think of these as stochastic changes in recruitment driven by environmental factors, which we could write as


$$(b_{t + 1} = b_{t} + b_{t}r(1 - \frac{ b_{t}}{k}) - c_{t})e^{normal(p_{t} |0,\sigma_{p})}$$

Meaning that the resulting "process errors" $p_{t}$ are lognormally distributed with a median of 1. 

Notice one weird thing about this.....

Let's try and code this up!

First things first

## Hake Data

```{r}

hake <- read.table(here::here("data","schaefer.dat"), header=TRUE)

hake <- hake %>% 
  set_names(tolower) %>% 
  mutate(year = 1:nrow(.))

hake %>% 
  gather('variable','index', -year) %>% 
  ggplot(aes(year, index, color = variable)) + 
  geom_point() + 
  facet_wrap(~variable, scales = 'free_y')
```

## Hake Fit

```{r}


n_chains <- 4

hake_data <- list(n_years = nrow(hake),
                  years = hake$year,
                  harvest = hake$catch,
                  index = hake$index)


initials <- list(log_r=-1.010073 , log_k= 7.973823, iq=2822.014, sigma_process=1, sigma_observation=1)
             # log_pop_devs=rep(0, len=nrow(hake)))

initials2 <- list(log_r=-0.5010073 , log_k= 3.973823, iq=500, sigma_process=1, sigma_observation=1) 
             # log_pop_devs=rep(0, len=nrow(hake)))

set.seed(22)
a <- Sys.time()
hake_fit <- stan(
  file = here::here("scripts","schaefer.stan"),
  data = hake_data,
  chains = 1,
  warmup = 10,
  iter = 50,
  cores = 1,
  refresh = 25,
  init = list(initials))


hake_fit <- stan(
fit = hake_fit,
data = hake_data,
  chains = n_chains,
  warmup = 1000,
  iter = 2000,
  cores = n_chains,
  refresh = 25,
seed = 42,
# init =  map(1:n_chains, ~map(initials, ~jitter(.x, 0)), initials = initials),
init = list(initials),
init =  map(1:n_chains, ~map(initials, ~jitter(.x, 0)), initials = initials),
control = list(max_treedepth = 15,
                adapt_delta = 0.99,
                adapt_engaged = TRUE ))

  # init = list(list(log_k = log(10))),

Sys.time() - a

```

## Hake Diagnostics

```{r extract-diagnostics}


hake_diagnostics <- rstan::get_sampler_params(hake_fit) %>% 
  set_names(1:n_chains) %>% 
   map_df(as_data_frame,.id = 'chain') %>% 
  group_by(chain) %>% 
  mutate(iteration = 1:length(chain)) %>% 
  mutate(warmup = iteration <= 1000)
 

hake_diagnostics %>% 
  group_by(warmup, chain) %>% 
  summarise(percent_divergent = mean(divergent__ >0)) %>% 
  ggplot() +
  geom_col(aes(chain, percent_divergent, fill = warmup), position = 'dodge', color = 'black') + 
  scale_y_continuous(labels = scales::percent)

```


plot results

```{r}

hake_summary <- summary(hake_fit)$summary %>% 
  as.data.frame() %>% 
  mutate(variable = rownames(.)) %>% 
  as_data_frame()

hake_fit_index <- hake_summary %>% 
  filter(str_detect(variable,'index')) %>% 
  mutate(year = str_replace_all(variable,'\\D','') %>% as.numeric()) %>% 
  mutate(variable = str_replace_all(variable,"(\\d)|(\\[)|(\\])","")) %>% 
  select(mean, variable, year) %>% 
  spread(variable, mean) %>% 
  mutate(true_index = hake$index)


biomass <- hake_summary %>% 
  filter(str_detect(variable,'biomass')) %>% 
  mutate(year = str_replace_all(variable,'\\D','') %>% as.numeric()) %>% mutate(year = hake$year)

b_and_c <- biomass %>% 
  ggplot() + 
  geom_line(aes(year, mean), linetype = 2, color = 'red') + 
  geom_ribbon(aes(year, ymin = `2.5%`, ymax = `97.5%`), alpha = 0.25, fill = 'red') + 
  geom_line(data = hake, aes(year, catch))


fit_plot <- hake_fit_index %>% 
  select(year, true_index,index_hat) %>% 
  ggplot() + 
  geom_line(aes(year, index_hat)) + 
  geom_point(aes(year, true_index))

d <- b_and_c + fit_plot + plot_layout(ncol = 2, nrow = 1)

 d


```

```{r}

hake_pop_devs <- hake_summary %>% 
  filter(str_detect(variable,'log_pop_devs')) %>% 
  mutate(year = str_replace_all(variable,'\\D','') %>% as.numeric()) %>% 
  mutate(variable = str_replace_all(variable,"(\\d)|(\\[)|(\\])","")) %>% 
  select(mean, variable, year) %>% 
  spread(variable, mean) %>% 
  mutate(true_index = hake$index)

hake_pop_devs %>% 
  ggplot(aes(year, log_pop_devs)) + 
  geom_line()

```

```{r}

hake_pop_devs <- hake_summary %>% 
  filter(str_detect(variable,'biomass')) %>% 
  mutate(year = str_replace_all(variable,'\\D','') %>% as.numeric()) %>% 
  mutate(variable = str_replace_all(variable,"(\\d)|(\\[)|(\\])","")) %>% 
  select(mean, variable, year) %>% 
  spread(variable, mean) %>% 
  mutate(true_index = hake$index)

hake_pop_devs %>% 
  ggplot(aes(year, biomass)) + 
  geom_line()

```

```{r}

hake_pop_devs <- hake_summary %>% 
  filter(str_detect(variable,'population')) %>% 
  mutate(year = str_replace_all(variable,'\\D','') %>% as.numeric()) %>% 
  mutate(variable = str_replace_all(variable,"(\\d)|(\\[)|(\\])","")) %>% 
  select(mean, variable, year) %>% 
  spread(variable, mean) %>% 
  mutate(true_index = hake$index)

hake_pop_devs %>% 
  ggplot(aes(year, population)) + 
  geom_line()

```


# Hierarchical Regression Models



# `rstanarm`

The process we've worked on here can be applied to just about any level of problem, and is especially useful for non-linear problems. 

However, a lot of the time, we just want to run regressions, albeit maybe more complex ones like `glms`, `glmers`, `gams`, etc. We could of course code these up manually, but the nice folks at Stan have conveniently put together a package called `rstanarm`, that provides built in functionality for running most forms of linear regression in Stan. This makes it really easy to run regression in a Bayesian manner, either because you have explicit priors your want to incorporate, or because you simply prefer the statistical interpretation. 

running `rstanarm` is so simple that we don't spend time on it here, and the documentation is very good.  You can run `help(exmaple_model)` to see an example of it's use, but basically you write your linear regression in the exact same way as if you were using `lm`, `glm`, or `glmer`, or whatever, but just pass it to the `stan_` version of it. 

```{r}
example_model <- 
  stan_glmer(cbind(incidence, size - incidence) ~ size + period + (1|herd),
             data = lme4::cbpp, family = binomial,
             # this next line is only to keep the example small in size!
             chains = 2, cores = 1, seed = 12345, iter = 500)
example_model
```

You can then run all the same diagnostics that we've gone through in this workshop to check whether your model has converged, examine posterior predictives, etc. 

# `TMB`

I would be remise not to spend a moment mentioning Template Model Builder (TMB), since on a recent project of mine I was forced to abandon Stan in favor of TMB... 


